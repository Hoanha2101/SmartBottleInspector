{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4961e54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.18\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea3ead0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.1.15'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6801a5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9627ba4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Laptop GPU'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30beac26",
   "metadata": {},
   "source": [
    "# Detect, track and count Persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c23349aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: 'APP/Tracking-and-counting-Using-YOLOv8-and-DeepSORT'\n",
      "d:\\semester_5_AI_doc\\Deep_learning_project\\UI - Hoan\\APP\\Tracking-and-counting-Using-YOLOv8-and-DeepSORT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\cuda\\lib\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd APP/Tracking-and-counting-Using-YOLOv8-and-DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ac57944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 d:\\semester_5_AI_doc\\Deep_learning_project\\UI - Hoan\\APP\\Tracking-and-counting-Using-YOLOv8-and-DeepSORT\\bottle_728.jpg: 480x640 1 person, 1 bottle, 176.6ms\n",
      "Speed: 1.0ms preprocess, 176.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict2\u001b[0m\n",
      "[39.0, 0.0]\n",
      "Class: bottle\n",
      "Class: person\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8m.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "results = model(\"bottle_728.jpg\", save=True)\n",
    "\n",
    "class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bbox outputs\n",
    "    probs = result.probs  # Class probabilities for classification outputs\n",
    "    cls = boxes.cls.tolist()  # Convert tensor to list\n",
    "    xyxy = boxes.xyxy\n",
    "    xywh = boxes.xywh  # box with xywh format, (N, 4)\n",
    "    conf = boxes.conf\n",
    "    print(cls)\n",
    "    for class_index in cls:\n",
    "        class_name = class_names[int(class_index)]\n",
    "        print(\"Class:\", class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c7b6e",
   "metadata": {},
   "source": [
    "# DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "945f584b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d74f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the video path\n",
    "video_path = 'bottle_2.avi'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_path = 'output.mp4'\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09056afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "533ff5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\cuda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m og_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     41\u001b[0m frame \u001b[38;5;241m=\u001b[39m og_frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 43\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m39\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     45\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mboxes  \u001b[38;5;66;03m# Boxes object for bbox outputs\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cuda\\lib\\site-packages\\ultralytics\\engine\\model.py:155\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cuda\\lib\\site-packages\\ultralytics\\engine\\model.py:399\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m)(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cuda\\lib\\site-packages\\ultralytics\\engine\\predictor.py:341\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model, verbose)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cuda\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:141\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, fuse, verbose)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nn_module:  \u001b[38;5;66;03m# in-memory PyTorch model\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     model \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 141\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m fuse \u001b[38;5;28;01melse\u001b[39;00m model\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    143\u001b[0m         kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:178\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Conv2):\n\u001b[0;32m    177\u001b[0m     m\u001b[38;5;241m.\u001b[39mfuse_convs()\n\u001b[1;32m--> 178\u001b[0m m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_conv_and_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update conv\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mdelattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# remove batchnorm\u001b[39;00m\n\u001b[0;32m    180\u001b[0m m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward_fuse  \u001b[38;5;66;03m# update forward\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cuda\\lib\\site-packages\\ultralytics\\utils\\torch_utils.py:189\u001b[0m, in \u001b[0;36mfuse_conv_and_bn\u001b[1;34m(conv, bn)\u001b[0m\n\u001b[0;32m    187\u001b[0m w_conv \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mview(conv\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    188\u001b[0m w_bn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(bn\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdiv(torch\u001b[38;5;241m.\u001b[39msqrt(bn\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m+\u001b[39m bn\u001b[38;5;241m.\u001b[39mrunning_var)))\n\u001b[1;32m--> 189\u001b[0m fusedconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mcopy_(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_bn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_conv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(fusedconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Prepare spatial bias\u001b[39;00m\n\u001b[0;32m    192\u001b[0m b_conv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from deep_sort.utils.parser import get_config\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "from deep_sort.sort.tracker import Tracker\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "import colorsys\n",
    "import numpy as np\n",
    "\n",
    "from library import *\n",
    "from utils import *\n",
    "\n",
    "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7'\n",
    "tracker = DeepSort(model_path=deep_sort_weights, max_age=70)\n",
    "\n",
    "\n",
    "frames = []\n",
    "\n",
    "unique_track_ids = set()\n",
    "i = 0\n",
    "counter, fps, elapsed = 0, 0, 0\n",
    "start_time = time.perf_counter()\n",
    "cap = cv2.VideoCapture(\"bottle_2.avi\")\n",
    "\n",
    "model = YOLO(\"yolov8m.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "while cap.isOpened():\n",
    "    start = time.time()\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        \n",
    "        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        frame = og_frame.copy()\n",
    "\n",
    "        results = model(frame, device=0, classes=39, conf=0.9, verbose = False)\n",
    "        for result in results:\n",
    "            boxes = result.boxes  # Boxes object for bbox outputs\n",
    "            probs = result.probs  # Class probabilities for classification outputs\n",
    "            cls = boxes.cls.tolist()  # Convert tensor to list\n",
    "            xyxy = boxes.xyxy\n",
    "            conf = boxes.conf\n",
    "            xywh = boxes.xywh  # box with xywh format, (N, 4)\n",
    "            for class_index in cls:\n",
    "                class_name = class_names[int(class_index)]\n",
    "                print(\"Class:\", class_name)\n",
    "\n",
    "        pred_cls = np.array(cls)\n",
    "        conf = conf.detach().cpu().numpy()\n",
    "        xyxy = xyxy.detach().cpu().numpy()\n",
    "        bboxes_xywh = xywh\n",
    "        bboxes_xywh = xywh.cpu().numpy()\n",
    "        bboxes_xywh = np.array(bboxes_xywh, dtype=float)\n",
    "        \n",
    "        tracks = tracker.update(bboxes_xywh, conf, og_frame)\n",
    "        \n",
    "        for track in tracker.tracker.tracks:\n",
    "            track_id = track.track_id\n",
    "            hits = track.hits\n",
    "            x1, y1, x2, y2 = track.to_tlbr()  # Get bounding box coordinates in (x1, y1, x2, y2) format\n",
    "            w = x2 - x1  # Calculate width\n",
    "            h = y2 - y1  # Calculate height\n",
    "\n",
    "            # Set color values for red, blue, and green\n",
    "            red_color = (0, 0, 255)  # (B, G, R)\n",
    "            blue_color = (255, 0, 0)  # (B, G, R)\n",
    "            green_color = (0, 255, 0)  # (B, G, R)\n",
    "\n",
    "            # Determine color based on track_id\n",
    "            color_id = track_id % 3\n",
    "            if color_id == 0:\n",
    "                color = red_color\n",
    "            elif color_id == 1:\n",
    "                color = blue_color\n",
    "            else:\n",
    "                color = green_color\n",
    "            print((int(x1), int(y1)), (int(x1 + w), int(y1 + h)))\n",
    "            cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
    "            \n",
    "            \n",
    "            \n",
    "            cv2.putText(og_frame, f\"{class_name}-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Add the track_id to the set of unique track IDs\n",
    "            unique_track_ids.add(track_id)\n",
    "            \n",
    "            region_bottle = og_frame[int(y1):int(y1+h),int(x1):int(x1+w)]\n",
    "            print(region_bottle.shape)\n",
    "            # result_cls = predict_cnn_bottle_RT(region_bottle)\n",
    "            \n",
    "            # cv2.putText(og_frame, str(result_cls), (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX , 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Update the person count based on the number of unique track IDs\n",
    "        person_count = len(unique_track_ids)\n",
    "\n",
    "        # Update FPS and place on frame\n",
    "        current_time = time.perf_counter()\n",
    "        elapsed = (current_time - start_time)\n",
    "        counter += 1\n",
    "        if elapsed > 1:\n",
    "            fps = counter / elapsed\n",
    "            counter = 0\n",
    "            start_time = current_time\n",
    "\n",
    "        # Draw person count on frame\n",
    "        cv2.putText(og_frame, f\"Person Count: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Append the frame to the list\n",
    "        frames.append(og_frame)\n",
    "        end = time.time()\n",
    "        # Tính FPS\n",
    "        fps = 1.0 / (end - start)\n",
    "\n",
    "        # Hiển thị FPS lên khung hình\n",
    "        cv2.putText(og_frame, f'FPS: {fps:.2f}', (10, 450), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Video\", og_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
